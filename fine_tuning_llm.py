# -*- coding: utf-8 -*-
"""Fine-tuning_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mIrO9mQ8qBkZ9IVtDzAmvBTyRP2fivYA
"""

import os
import subprocess
import warnings
import torch

# ====================================================
# 1Ô∏è‚É£ Install Required Packages (for VS Code)
# ====================================================
def install_packages():
    packages = [
        "transformers", "datasets", "peft", "trl",
        "accelerate", "safetensors", "huggingface_hub"
    ]
    for pkg in packages:
        try:
            __import__(pkg)
        except ImportError:
            print(f"Installing missing package: {pkg}")
            subprocess.check_call(["pip", "install", pkg, "--quiet"])

install_packages()

# ====================================================
# 2Ô∏è‚É£ Imports
# ====================================================
from huggingface_hub import login
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer
from peft import LoraConfig, TaskType

warnings.filterwarnings("ignore")
torch.set_grad_enabled(True)

# ====================================================
# 3Ô∏è‚É£ Hugging Face Login
# ====================================================
try:
    login()  # Paste your Hugging Face token when prompted
except Exception as e:
    print(f"‚ö†Ô∏è Hugging Face login skipped: {e}")

# ====================================================
# 4Ô∏è‚É£ Load Dataset
# ====================================================
dataset_path = "people_osl.jsonl"  # Use local path for VS Code
if not os.path.exists(dataset_path):
    raise FileNotFoundError(f"‚ùå Dataset not found: {dataset_path}")

dataset = load_dataset("json", data_files=dataset_path, split="train")
print("‚úÖ Sample data:", dataset[0])

# ====================================================
# 5Ô∏è‚É£ Load Tokenizer & Model
# ====================================================
model_id = "meta-llama/Llama-3.2-3B-Instruct"  # Ensure you have HF access

print("üîÑ Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    trust_remote_code=True
)

# ====================================================
# 6Ô∏è‚É£ LoRA Config
# ====================================================
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# ====================================================
# 7Ô∏è‚É£ Formatting Function
# ====================================================
def formatting_func(example):
    return f"### Instruction:\n{example['instruction']}\n### Response:\n{example['output']}\n"

training_args = TrainingArguments(
    output_dir="./llama32_lora_output",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=1e-4,
    fp16=torch.cuda.is_available(),
    logging_steps=80,
    max_steps=800,
    save_strategy="steps",
    save_steps=200,
    save_total_limit=1,
    optim="adamw_torch",
    lr_scheduler_type="constant",
    report_to=[]
)

# ====================================================
# 8Ô∏è‚É£ Initialize SFTTrainer
# ====================================================
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=lora_config,
    formatting_func=formatting_func,
    args=training_args
)

# ====================================================
# 9Ô∏è‚É£ Train & Save
# ====================================================
print("üöÄ Starting training...")
trainer.train()
save_path = "./llama32_lora_merged_exact"
trainer.save_model(save_path)
print(f"‚úÖ Training complete. Model saved at {save_path}")

# ====================================================
# üîü Clean Up Old Folders (optional)
# ====================================================
import shutil
folders_to_delete = [
    "./llama32_lora_output",
    "./llama32_lora_merged",
    "./llama32_lora_merged_exact"
]
for folder in folders_to_delete:
    shutil.rmtree(folder, ignore_errors=True)
    print(f"üßπ Folder '{folder}' deleted (if existed).")

# ====================================================
# üß† LLaMA 3.2 LoRA Inference with MCP Integration
# ====================================================

# ====================================================
# 1Ô∏è‚É£ Load Model + Tokenizer
# ====================================================
print("\nüîÑ Loading fine-tuned model for inference...")
model_path = "./llama32_lora_merged_exact"
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
)
model.eval()

# ====================================================
# 2Ô∏è‚É£ Load Knowledge Base
# ====================================================
data_path = "people_osl.jsonl"
if not os.path.exists(data_path):
    raise FileNotFoundError(f"‚ùå Dataset not found at {data_path}")

dataset = load_dataset("json", data_files=data_path, split="train")
knowledge_base = {d["instruction"].strip().lower(): d["output"].strip() for d in dataset}
print(f"‚úÖ MCP Knowledge Base Loaded: {len(knowledge_base)} entries")

# ====================================================
# 3Ô∏è‚É£ MCP Core
# ====================================================
class MCPServer:
    """Lightweight Model Control Processor (MCP)"""

    def __init__(self, model, tokenizer, knowledge_base):
        self.model = model
        self.tokenizer = tokenizer
        self.kb = knowledge_base

    def database_lookup(self, instruction: str):
        return self.kb.get(instruction.strip().lower())

    def generate_model_response(self, instruction: str, max_new_tokens=200):
        prompt = f"### Instruction:\n{instruction}\n### Response:\n"
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                temperature=0.0,
                top_p=1.0,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id
            )
        response = self.tokenizer.decode(
            output_ids[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        return response.strip()

    def process_request(self, instruction: str):
        db_result = self.database_lookup(instruction)
        if db_result:
            source = "‚úÖ MCP Database"
            response = db_result
        else:
            source = "üß© Model"
            response = self.generate_model_response(instruction)
        if not response:
            response = "‚ö†Ô∏è Unable to generate a valid response."
            source = "‚ö†Ô∏è Guardrail triggered"
        return source, response


# ====================================================
# 4Ô∏è‚É£ Run Interactive MCP Loop
# ====================================================
mcp = MCPServer(model, tokenizer, knowledge_base)
print("\n‚úÖ LLaMA 3.2 LoRA with MCP ready.\n-----------------------------------------")

while True:
    user_input = input("Enter instruction (or 'exit' to quit): ").strip()
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Exiting Inference...")
        break
    source, response = mcp.process_request(user_input)
    print(f"\n[{source}]\n{response}\n")
    print("-----------------------------------------\n")
